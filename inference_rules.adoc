:toc:
:toclevels: 4

:sectnums:

= MLPerf Inference Rules
Version 0.5
April 16th, 2019

Points of contact: David Kanter (dkanter@gmail.com), Vijay Janapa Reddi (vjreddi@g.harvard.edu)

This document is stale with respect to the inference rules, please consult https://docs.google.com/document/d/1PUYLYr4eENzrsAM4XoeJY17ehmhdEpVxaFjhcGxO9xc/edit# for the most recent rules.

== Overview
This document describes how to implement one or more benchmark in the MLPerf Inference Suite and how to use that implementation to measure the performance of an an ML system performing inference.

The MLPerf name and logo are trademarks. In order to refer to a result using the MLPerf name, the result must conform to the letter and spirit of the rules specified in this document. The MLPerf organization reserves the right to solely determine if a use of its name or logo is acceptable.

=== Definitions (read this section carefully)
The following definitions are used throughout this document:

_Performance_ always refers to a scenario-specific metric for inference queries measured using the MLPerf load generator.

A _sample_ is the unit on which inference is run. E.g., an image, or a sentence.

A _query_ is a set of N samples that are issued to an inference system together. N is a positive integer. For example, a single query contains 8 images.

_Quality_ always refers to a model’s ability to produce “correct” outputs.

A _system under test_ consists of a defined set of hardware and software resources that will be measured for perforamnce.  The hardware resources may include processors, accelerators, memories, disks, and interconnect. The software resources may include an operating system, compilers, libraries, and drivers that significantly influences the running time of a benchmark.

A _framework_ is a specific version of a software library or set of related libraries, possibly with associated offline compiler, for training and/or executing ML models using a system. Examples include specific versions of Caffe2, MXNet, PaddlePaddle, pyTorch, or TensorFlow.

A _graph compiler_ is ??

A _task_ is a generalized, high-level, user-understandable problem that is comprised of one or more ML models and includes pre- and post-processing steps for the models.
  
A _benchmark_ is a measurable task that can be solved by executing a trained model using a specific input dataset at a target quality level and one or more target latency levels.

A _suite_ is a specific set of benchmarks.
A _division_ has a set of rules for implementing a suite to produce a class of comparable results.

A _reference implementation_ is a specific implementation of a benchmark provided by the MLPerf organization.  The reference implementation is the canonical implementation of a benchmark. All valid submissions of a benchmark must be *equivalent* to the reference implementation.

A _benchmark implementation_ is an implementation of a benchmark in a particular framework by a user under the rules of a specific division.

A _suite implementation_ is a set of benchmark implementations for the entire suite using the same framework under the rules of a specific division.

A _run_ is a complete execution of a benchmark implementation on a system, executing a model using specific input data with a target quality level and target latency. (BRING INTO ALIGNMENT WITH SECTION BELOW)

An _run result_ consists of completing a set of inference queries, including data pre- and post-processing, meeting a latency requirement and a quality requirement.  Each benchmark has a quality requirement and a 

A _reference result_ is a run result provided by the MLPerf organization for each reference implementation on a reference system.

A _benchmark result_ is a run result normalized to the reference result for that benchmark. Normalization is of the form (reference result * constant / benchmark result) such that a better benchmark result produces a higher number.  The constant shall be selected to ensure that most benchmark results are greater than 1. (FIX ME)

== General rules
The following rules apply to all benchmark implementations.

=== Strive to be fair
Benchmarking should be conducted to measure the framework and system performance as fairly as possible. Ethics and reputation matter.

=== System and framework must be consistent
The same system and framework must be used for a suite result or set of benchmark results reported in a single context.

Note that the reference implementations use different frameworks and hence cannot be used collectively for a valid suite result.

=== System and framework must be available
If you are measuring the performance of a publicly available and widely-used system or framework, you must use publicly available and widely-used used versions of the system or framework.

If you are measuring the performance of an experimental framework or system, you must make the system and framework you use available upon demand for replication.

=== Benchmark implementations must be shared
Source code used for the benchmark implementations must be open-sourced under a license that permits a commercial entity to freely use the implementation for benchmarking. The code must be available as long as the results are actively used.

=== Non-determinism is restricted
The only forms of acceptable non-determinism are:

* Floating point operation order
* Random traversal of the inputs
* Rounding

All random numbers must be drawn from the framework’s stock random number generator. The random number generator seed must entirely determine its output sequence. Random numbers must be utilized in a logical and consistent order across runs. Random number generators may be seeded from the following sources:

* Clock
* System sources of randomness, e.g., /dev/random or /dev/urandom
* Another random number generator initialized with an allowed seed

Additional rules may apply as described in later sections.

=== Benchmark detection is not allowed
The framework and system should not detect and behave differently for benchmarks.

=== Input-based optimization is not allowed
The implementation should not encode any information about the content of the input dataset in any form.

=== Replicability is mandatory
Results that cannot be replicated are not valid results.

== Benchmarks
The MLPerf organization provides a reference implementation of each benchmark, which includes the following elements:
Code that implements the model in a framework.
A plain text “README.md” file that describes:

* Problem
** Dataset/Environment
** Publication/Attribution
** Data pre- and post-processing
** Performance, accuracy, and calibration data sets
** Test data traversal order (CHECK)
* Model
** Publication/Attribution
** List of layers
** Weights and biases
* Quality and latency
** Quality target
** Latency target(s)
* Directions
** Steps to configure machine
** Steps to download and verify data
** Steps to run and time

A “download_dataset” script that downloads the accuracy, speed, and calibration datasets.

A “verify_dataset” script that verifies the dataset against the checksum.

A “run_and_time” script that executes the benchmark and reports the wall-clock time.

=== Benchmarks
The benchmark suite consists of the benchmarks shown in the following table.

|===
|Area |Task |Model |Dataset |Quality |Latency constraint
|Vision |Image classification |Resnet50-v1.5 |ImageNet (224x224) |74.9% top-1 |99% @ 10ms, 50ms, 100ms, 200ms 
|Vision |Image classification |MobileNets-v1 224 |ImageNet  (224x224) |?? |?? 
|Vision |Object detection |SSD-ResNet34 |COCO (1200x1200) |0.212 mAP |?? 
|Vision |Object detection |SSD-MobileNets-v1 |COCO (300x300) |?? |?? 
|Language/Audio |Machine translation |GMNT |WMT16 |22 uncased BLEU |?? 
|===

== Load Generator
The MLPerf provided load generator (LoadGen) controls and initates inference queries to the SUT. The LoadGen is provided in C++ with Python bindings and must be used. The LoadGen operates in two modes: accuracy and performance.

ACCURACY: Accuracy mode is intended to measure the quality of the submission and ensure that it meets or exceeds the specified quality target. Inference queries are initiated by the LoadGen to measure quality of the system on a quality data set. The result of accuracy mode is either PASS or FAIL and accuracy mode is not timed.

PERFORMANCE: Performance mode is intended to measure the performance of the submission on the selected scenario(s). Input data for inference queries begins in system memory. In principle, system memory is the memory where the operating system resides. In nearly every case the system memory should correspond to commodity DRAM (e.g., DDRx or LPDDRx) attached to the host CPU. Inference queries are initiated by the LoadGen in accordance with a selected scenario(s). Inference queries are timed to calculate performance metric(s) in accordance with the selected scenario(s).

== Scenarios
In order to enable representative testing of a wide variety of inference platforms and use cases, MLPerf has defined four different scenarios as described in the table below.

|===
|Scenario |Query Generation |Duration |Inferences/query |Latency Constraint |Tail Latency | Performance Metric
|Single stream |New query as soon as SUT completes the current query |max {1024 queries, 60 seconds} |1 |None |90% | 90%-ile measured latency 
|Multiple stream |New query every _latency constraint_ if the SUT has completed the current query, otherwise the new query is dropped and is counted as one overtime query |max {24K queries, 60 seconds} |Variable, see metric |Benchmark specific |90% | Maximum number of inferences per query supported
|Server |New queries according to Poisson distribution, overtime queries may be queued and processed at end |max {24K queries, 60 seconds} |1 |Benchmark specific |90% | Maximimum Poisson throughput parameter supported
|Offline |All queries available at start |max {24K queries, 60 seconds} |All |None |N/A | Measured throughput
|===

A submission may compise any combination of benchmark and scenario results.

== Reference Systems
The reference systems are the MLPerf developer target platforms.

MLPerf guarantees that each of the cloud/edge reference implementations will achieve the required accuracy on the appropriate cloud/edge reference system.  All submissions must be equivalent to the reference implementation on the reference system, as described in this document.

The reference systems are selected for ease of development and are used as an arbitrary baseline used to compute relative performance of submissions.  The reference systems are not intended to be reflective of any particular market, application, or deployment.

=== Cloud Reference System (UPDATE ME)
The cloud reference platform is a Google Compute Platform n1-highmem-16 (16 vCPUs, 104GB memory) instance using the Skylake processor generation.

MLPerf guarantees that the reference implementations of all cloud benchmarks will run on the cloud reference system.

=== Edge Reference System
The edge reference system is an Intel NUC 7 Home (NUC7i3BNHXF):

* Core i3-7100U Processor (dual-core, four-thread Kaby Lake, 2.4GHz base)
* 4GB of DDR4 memory 
* 16GB of Optane memory (3DXP connected via PCIe)
* 1TB SATA hard drive
* Running Ubuntu 16.04

MLPerf guarantees that the reference implementations of all edge benchmarks will run on the edge reference system. The reference system can be obtained via Amazon and the hardware cost is $400.

== Divisions
There are two divisions of the benchmark suite, the Closed division and the Open division.

=== Closed Division
The Closed division requires using pre-processing, post-processing, and model that is equivalent to the reference or alternative implementation.  The closed division allows calibration for quantization and does not allow any retraining.

The unqualified name “MLPerf” must be used when referring to a Closed Division suite result, e.g. “a MLPerf result of 4.5.”

=== Open Division
The Open division allows using arbitrary pre- or post-processing and model, including retraining.
The qualified name “MLPerf Open” must be used when referring to an Open Division suite result, e.g. “a MLPerf Open result of 7.2.”

== Data Sets
=== Data State at Start of Run
Each reference implementation includes a script to download the accuracy, speed, and calibration datasets and a script to verify the datasets using a checksum. The dataset must be unchanged at the start of each run.

=== Pre- and post-processing
All imaging benchmarks take uncropped uncompressed bitmap as inputs, NMT takes text. 

CLOSED: The same pre- and post-processing steps as the reference implementation must be used. Additional pre- and post-processing is not allowed.

OPEN: Any pre- and post-processing steps are allowed. Each datum must be preprocessed individually in a manner that is not influenced by any other data.

CLOSED and OPEN: Sample-independent pre-processing that matches the reference model is untimed. However, it must be pre-approved and added to the following list:

* May resize to processed size (e.g. SSD-large)
* May reorder channels / do arbitrary transpositions
* May pad to arbitrary size (don’t be creative)
* May do a single, consistent crop
* Mean subtraction and normalization provided reference model expect those to be done
* May quantize image data from fp32 to int8 and between signed and unsigned

Any other pre- and post-processing time (e.g., for OPEN) is included in the wall-clock time for a run result.

=== Test Data Traversal Order (UPDATE ME)
Test data DEFINE PER SCENARIO.  Batch size may affect order.

Future versions of the benchmark suite may specify the traversal order.

== Model
CLOSED: For v0.5, MLPerf provides a reference implementation in a first framework and an alternative implementation in a second framework in accordance with the table below.  The benchmark implementation must use a model that is equivalent to the reference implementation or the alternative implementation, as defined by the remainder of this section.

|===
|Area |Task |Model |Reference implementation |Alternative implementation
|Vision |Image classification |Resnet50-v1.5 |TF |PyTorch/ONNX 
|Vision |Image classification |MobileNets-v1 224 |TensorFlow/TensorFlow Lite |PyTorch/ONNX  
|Vision |Object detection |SSD-ResNet34 |PyTorch/ONNX |TensorFlow/TensorFlow Lite 
|Vision |Object detection |SSD-MobileNets-v1 |TensorFlow |PyTorch/ONNX 
|Language/Audio |Machine translation |GMNT |TensorFlow |PyTorch/ONNX 
|===

OPEN: The benchmark implementation may use a different model to perform the same task. Retraining is allowed.

=== Graph Definition
CLOSED: The reference and alternative implementations each have a graph that describes the operations performed during inference. Benchmark implementations must choose and specify the reference or alternative and the same graph.

OPEN: Benchmark implementations may use a different graph compared to the reference or alternative implementation.

=== Weight Definition and Quantization
CLOSED: MLPerf wil provide trained weights and biases in fp32 format for both the reference and alternative implementation.  MLPerf will also provide a calibration data set. Submitters may do arbitrary purely mathematical, reproducible public method quantization using only the calibration data and weight and bias tensors from the model to any format that achieves the desired quality. 
Additionally, for image classification using MobileNets-v1 224 and object detection using SSD-MobileNets-v1, MLPerf will provide a retrained int8 (comprising 127 positive, 127 negative, and precise zero) model in two's complement format in a JSON container. Model weights and input activations are scaled per tensor, and must preserve the same shape modulo padding. Convolution layers are allowed to be in either NCHW or NHWC format.  No other retraining is allowed.

OPEN: Weights and biases must be initialized to the same values for each run.

=== Graph Execution
CLOSED: Graph compilers are free to optimize the “non-stateful” parts of the computation graph provided that the semantics are unchanged. So optimizations and graph / code transformations of the flavor of layer fusion, dead code elimination, common subexpression elimination, and loop-invariant code motion are entirely allowed.

OPEN: Frameworks are free to alter the graph.

=== Hyperparameters
Hyperparameters (e.g. batch size) may be selected to best utilize the framework and system being tested, given the quality and latency requirements.

== System Reporting
Cloud and edge benchmarks may be run both on either hardware as a service or physical hardware.

=== With Hardware as a Service
==== Replication recipe
Report a recipe that starts from a vanilla VM image or Docker container and a sequence of steps that creates the system that performs the benchmark measurement.

==== Price
Include the total cost of obtaining the median run result using fixed prices for the general public at the time the result is collected. Do not use spot pricing.

=== With Physical Hardware
==== Replication recipe
Report everything that will eventually be required by a third-party user to replicate the result when the hardware and software becomes widely available.

==== Power
For v0.5, power measurement is optional, but should be in accordance with recommendations if performed.  As per all performance testing, we expect that power measurements will be reproducible.

* Power is measured for a “device under test” (DUT)
**The DUT may be wall-powered or battery-powered
* The DUT for v0.5 is a full system that is capable (without external assistance) of:
** Receiving input data (e.g., via network or I/O)
** Pre-processing (e.g., via DSP, CPU)
** Performing inference (e.g., via CPU, GPU, accelerator)
** Post-processing (e.g., via GPU), and
** Any other step deemed necessary
*** Example DUTs include a smartphone, a server, a server with a PCIe accelerator, a PC with an accelerator USB stick.
* Metrics
**Energy and power are recorded and reported for the entirety of the performance test (e.g., including pre-/post-processing) at clearly defined boundaries. The power measurement must report:
*** Total energy consumed by the DUT
*** Peak power draw by the DUT (note that peak power draw is typically a microsecond-level granularity event)
* Measurement equipment
** We do not specify, but recommend following SPECpower recommendations for wall-powered devices and using https://www.msoon.com/online-store/High-Voltage-Power-Monitor-HVPM-p90002590 for mobile devices
* The submitted shall report:
** Complete configuration of DUT, and where/how power is measured
** Hardware and software used to gather the measurements
** Detailed instructions to experimentally reproduce the numbers

For recommendations and discussion of power management, please see https://docs.google.com/document/d/1XdX5-PHFuckeZYUJpEupvOgPmn_wmOHPY3JLP8-fjLs/.

== Submissions
The MLPerf organization will create a database that collects submission data; one feature of the database is producing a leaderboard.

=== Submission Form
Submissions to the database must use the provided submission form to report all required information.

=== Submission Process
Submit the completed form and supporting code to the MLPerf organization Github mlperf/results repo as a PR.

== FAQ
Q: Why does MLPerf specify the test data order?

A: Many systems will use batching to perform inference on multiple inputs. 


Q: Do I have to use the reference implementation framework?

A: No, you can use another framework provided that it matches the reference in the required areas.


Q: Do I have to use the reference implementation scripts?

A: No, you don’t have to use the reference scripts. The reference is there to settle conformance questions - with a few exceptions, a submission to the closed division must match what the reference is doing.


Q: What is the reference system? Do I have to use the reference system?

A: A reference system is a hardware and software platform that is guaranteed by MLPerf to run one or more benchmarks.  You can and should use different hardware and software configurations.  The reference hardware systems were chosen as development targets for MLPerf benchmarks and are not intended to be representative of any particular class of system.


Q: Can I run an edge benchmark on a server in a data center?  Can I run a cloud benchmark on a smartphone?

A: Either combination is allowed.


Q: Can I perform computations for inference using my favorite data types (int8, int4, IEEE fp16, bfloat16, etc.)?

A: We allow any data types to be used. However, the submission must achieve the required accuracy level in a reproducible manner.


Q: Why does a run require so many individual inference queries?

A: The numbers were selected to be sufficiently large to statistically verify that the system meets the latency requirements. 


Q: What information should I submit about the software of the system under test?

A: The goal is reproducibility.  At a minimum, a submission should include the OS and version number, software libraries and versions used, frameworks, etc.


Q: For my submission, I am going to use a different model format (e.g., ONNX vs TensorFlow Lite).  Should the conversion routine/script be included in the submission? Or is it sufficient to submit the converted model?

A: The goal is reproducibility, so you should include the conversion routine/scripts.
